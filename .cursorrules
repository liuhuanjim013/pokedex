# Pokemon Classifier Project Rules

## Google Colab Training Environment

### Environment Setup
1. **Conda Installation (Ephemeral)**
   - Install to: `/content/miniconda3/` (reinstalled per session)
   - Environment: `pokemon-classifier` with Python 3.9
   - Activation: `source conda.sh && conda activate pokemon-classifier`
   - Shell script: `activate_env.sh` for easy activation
   - No Google Drive persistence due to file permission issues

2. **Dynamic Path Management**
   - Conda path detection: Check `/content/miniconda3/bin/conda` first
   - Function: `get_conda_path()` in multiple scripts
   - Primary path: `/content/miniconda3/bin/conda`
   - Fallback: PATH environment variable detection
   - PATH updates: Automatic conda binary detection

3. **Storage Strategy**
   - Repository: `/content/pokedex/pokedex/` for code and configs
   - Dataset: `/content/pokedex/pokedex/data/yolo_dataset/` from Hugging Face
   - Models: `/content/models/` directories (ephemeral)
   - Auto-creation: Required directories (`models/checkpoints`, etc.)
   - Error handling: I/O error retry logic for large file operations

## YOLO Training Rules

### Data Configuration
1. **YOLO Dataset Format Requirements**
   - Directory: `processed/yolo_dataset/`
   - Images: 416x416 JPEG in `images/{split}/`
   - Labels: YOLO format in `labels/{split}/`
   - Naming: `{pokemon_id}_{image_number:03d}.{ext}` (3-digit padding)
   - Class IDs: 0-based (0-1024)
   - Full-image boxes: "0.5 0.5 1.0 1.0"
   - Multiprocessing: 8 workers, 100 images per batch
   - Progress Tracking: Percentage complete per split
   - Lookup Tables: O(1) access for raw-to-processed mapping

2. **Dataset Verification**
   - Check Hugging Face access first
   - Verify YOLO config format
   - Validate class count (1025)
   - Check split names match
   - Verify per-Pokemon 70/15/15 splits
   - Validate label format and class IDs (0-based)
   - Check image sizes (416x416)
   - Verify image data types (bytes or PIL)
   - Skip already processed splits
   - Show progress with tqdm
   - Handle errors with descriptive messages
   - Update config paths dynamically

### Model Loading Strategy
1. **YOLOv3 Loading Priority**
   - **Primary**: Load official YOLOv3 weights (Ultralytics auto-download)
   - **Fallback**: Load from Ultralytics hub (`YOLO("yolov3")`)
   - YAML fallback removed (file `models/configs/yolov3.yaml` deleted)
   - **Cache Management**: Auto-cleanup corrupted weights
   - **Path Resolution**: Dynamic working directory detection

2. **Training Configuration**
   - Classes: 1025 (all Pokemon generations)
   - Input size: 416x416 pixels
   - Learning rate: 1e-4, cosine schedule with 5 warmup epochs

### Baseline Training Results & Learnings
1. **Performance Metrics (48 epochs completed)**
   - **Best mAP50**: 0.9746 (epoch 44)
   - **Best mAP50-95**: 0.803 (epoch 44)
   - **Training Loss**: Steady decrease until epoch 45
   - **Validation Loss**: Increasing trend after epoch 30 (overfitting)

2. **Critical Issues Identified**
   - **Training Instability**: Dramatic performance drop at epoch 45
     - mAP50 dropped from 0.9746 to 0.00041
     - mAP50-95 dropped from 0.803 to 0.00033
   - **Overfitting**: Validation loss increasing while training loss decreasing
   - **Learning Rate**: 1e-4 may be too high for 1025 classes
   - **Augmentation**: Minimal augmentation insufficient for generalization

3. **Improvement Opportunities**
   - **Early Stopping**: Implement to prevent overfitting (patience=10)
   - **Learning Rate**: Reduce to 5e-5 or implement adaptive scheduling
   - **Augmentation**: Add rotation, shear, mosaic, mixup
   - **Regularization**: Add dropout, weight decay, label smoothing
   - **Batch Size**: Consider increasing to 32 for better gradient estimates
   - **Monitoring**: Add validation metrics monitoring for early detection

4. **Baseline Configuration (Current)**
   - Learning rate: 1e-4 (cosine schedule)
   - Batch size: 16
   - Warmup epochs: 5
   - Augmentation: Horizontal flip only (0.5 probability)
   - No early stopping
   - No additional regularization

### Training Process
1. **Initialization Order**
   - Verify environment setup and conda activation
   - Check dataset access from Hugging Face
   - Initialize W&B tracking with run resumption
   - Set up model with fallback loading strategy
   - Create required directories automatically

2. **Checkpoint Management**
   - Save metadata with checkpoints (W&B run ID, epoch, metrics)
   - Track W&B run ID for resumption
   - Record actual progress vs saved epochs
   - Handle mid-epoch interruptions
    - Local storage (ephemeral per Colab session)
    - Auto-backup of training outputs to Google Drive every 30 minutes
    - Resume scans multiple default paths for latest checkpoint:
      - Custom checkpoints directory
      - `pokemon-classifier/<run-name>/weights`
      - `pokemon-yolo-training/<run-name>/weights`

3. **Error Handling**
   - File I/O errors: Retry with exponential backoff for large operations
   - Missing conda: Dynamic path detection and helpful messages
   - Model loading failures: Multiple fallback strategies
   - Dataset access: Hugging Face authentication and caching

### W&B Integration
1. **Configuration**
   - Use singleton pattern for initialization
   - Enable offline fallback with environment variables
   - Disable code/git tracking for Colab
   - Use personal account (liuhuanjim013-self)
   - Project: "pokemon-classifier"

2. **Metrics Logging**
   - **Built-in Ultralytics**: Automatic loss, mAP, precision, recall
   - **Real-time Monitoring**: Live training progress (callbacks on epoch/val end)
   - **System Metrics**: GPU usage, memory consumption
   - **Custom Logging**: Configuration parameters and experiment metadata
   - **Run Persistence**: Save run ID to disk for resumption

3. **Resume Strategy**
   - Try specified checkpoint with matching W&B run ID
   - Match W&B run ID from saved checkpoint metadata
   - Fall back to latest checkpoint if run ID missing
   - Support forcing new run with --force-new-run flag
   - Maintain metrics continuity during resume

4. **Error Handling**
   - Clean up W&B runs on training failure
   - Proper exception handling with informative messages
   - Resource cleanup and session management
   - Offline mode fallback for network issues

### Testing Requirements
1. **Training Tests**
   - Verify setup works
   - Check arguments
   - Test data loading
   - Validate progress

2. **W&B Tests**
   - Test initialization
   - Verify resumption
   - Check offline mode
   - Validate cleanup

3. **Checkpoint Tests**
   - Test metadata saving
   - Verify loading
   - Check progress tracking
   - Test interruption recovery

4. **Dataset Format Tests**
   - Verify image sizes (416x416)
   - Check label format
   - Validate class IDs (0-based)
   - Verify per-Pokemon splits
   - Test image-label pairs match

## K210 Export Pipeline Rules

### K210 Export & Deployment Rules

1. **Model Size & Memory Constraints**
   - **Hardware Limits**:
     - RAM: ~6MB maximum
     - Flash: ~16MB maximum
   - **Target Metrics**:
     - Model size: 1-2MB
     - Runtime memory: 2-3MB
     - Input size: 224x224 or smaller
     - Architecture: Tiny/efficient variants

2. **nncase Version & API Rules**
   - **Version**: Use nncase v0.1.0-rc5 (MaixPy kmodel v3 compatibility)
   - **Critical Issue**: nncase v1.6.0+ generates kmodel v5 which is NOT compatible with MaixPy
   - **MaixPy Requirement**: Only supports kmodel v3, requires nncase v0.1.0-rc5
   - **API Approach**: Use Python API with version detection and fallbacks
   - **Compilation Flow**: 
     1. Detect nncase version
     2. Use v0.1.0-rc5 API for kmodel v3 generation
     3. Fallback to newer API with warnings
   - **Error Handling**: Try multiple API approaches based on version:
     1. v0.1.0-rc5 API with direct gencode
     2. Direct function calls if available
     3. Newer API as fallback (with compatibility warnings)

3. **Environment Setup Requirements**
   - **Installation**: Use `setup_environment.py` with `--k210-only` flag
   - **Dependencies**: Install onnx, onnxruntime, onnxsim, nncase
   - **Version Management**: Handle nncase version conflicts
   - **Verification**: Test compilation with tiny model

4. **Export Pipeline Requirements**
   - **ONNX Export**: Use Ultralytics with fixed input shape
   - **Calibration**: Prepare 400 images at target resolution
   - **Memory Analysis**: Report detailed memory usage breakdown
   - **Artifacts**: Package kmodel with classes.txt and anchors.txt

5. **Model Optimization Rules**
   - **Architecture**: Use YOLOv3-tiny or smaller variant
   - **Resolution**: Target 224x224 input size
   - **Classes**: Consider grouping similar Pokemon
   - **Optimization**: Use channel pruning and aggressive quantization
   - **Memory**: Optimize intermediate feature map sizes
   - **Batch Size**: Keep at 1 for inference
   - **Layers**: Use memory-efficient layer configurations

## K210 Training Pipeline Rules (IMPLEMENTED & WORKING)

### YOLOv3-Tiny-Ultralytics Training Results & Learnings (COMPLETED)
1. **Training Success (✅ ACHIEVED - 91.7% mAP50)**
   - **Model**: YOLOv3-tiny-ultralytics (yolov3-tinyu) - ✅ Successfully trained
   - **Input Size**: 224x224 (fixed, no dynamic sizing) - ✅ Applied
   - **Classes**: 1025 (ALL Pokemon generations 1-9) - ✅ Maintained
   - **Parameters**: 12.66M parameters (manageable for training, too large for K210)
   - **Final Performance**: 91.7% mAP50, 93.1% Precision, 84.0% Recall
   - **Critical Issue**: 49MB model exceeds K210 constraints (16MB Flash, 6MB RAM)

2. **Critical Training Insights (✅ MAJOR BREAKTHROUGH)**
   - **Initial Failure**: Conservative params (5e-5 LR, no augmentation) → 3.4% mAP50
   - **Parameter Optimization**: 20x LR increase + aggressive augmentation → 91.7% mAP50
   - **Key Learning**: 1025 classes require aggressive training approach
   - **Final Config**: LR=1e-3, SGD, mosaic=0.5, mixup=0.3, patience=10
   - **Success Factors**: Learning rate was primary bottleneck, augmentation prevented overfitting
   - **Training Stability**: 62 epochs without overfitting using aggressive parameters

3. **Data Strategy Rules (✅ SUCCESSFULLY IMPLEMENTED)**
   - **Dataset**: `liuhuanjim013/pokemon-yolo-1025` - ✅ Working
   - **No New Dataset**: Runtime resizing from 416x416 to 224x224 - ✅ Validated
   - **Full Coverage**: All 1025 Pokemon classes maintained - ✅ Confirmed
   - **Data Loading**: 90,126 train + 19,316 val images - ✅ Verified
   - **Memory Efficiency**: Through model architecture, not data reduction - ✅ Achieved

4. **Data Augmentation Rules (✅ CONSERVATIVE FOR K210)**
   - **Horizontal Flip**: 0.5 probability - ✅ Applied
   - **Rotation**: ±5° maximum (reduced for stability) - ✅ Applied
   - **Translation**: ±10% maximum (reduced for stability) - ✅ Applied
   - **Scale**: ±10% maximum - ✅ Applied
   - **Color Jitter**: Minimal (hsv_h=0.015, hsv_s=0.7, hsv_v=0.4) - ✅ Applied
   - **No Mosaic/Mixup**: Disabled (mosaic=0.0, mixup=0.0) - ✅ Applied

5. **K210 Hardware Compatibility Rules (🔄 READY FOR TESTING)**
   - **Activation**: Use ReLU6 (hardware optimized) - 🔄 To be applied during export
   - **Batch Norm**: Fuse with conv layers during export - 🔄 Ready
   - **Input Size**: Fixed 224x224 for K210 KPU - ✅ Applied
   - **Quantization**: INT8 post-training quantization - 🔄 Ready
   - **Memory Layout**: NCHW (K210 preferred) - ✅ Applied

6. **Training Infrastructure Rules (✅ FULLY IMPLEMENTED)**
   - **Resume Pattern**: Follows baseline/improved script pattern - ✅ Working
   - **W&B Integration**: Uses pokemon-classifier project - ✅ Working  
   - **Auto-backup**: Every 30 minutes to Google Drive - ✅ Working
   - **Command Line**: Full compatibility (--resume, --fresh, --checkpoint) - ✅ Working
   - **Error Handling**: Multi-layered approach for resume conflicts - ✅ Working
   - **Checkpoint Naming**: Consistent yolov3n_k210_optimized naming - ✅ Applied

7. **K210 Deployment Challenge (🚨 CRITICAL CONSTRAINT)**
   - **Export Success**: ✅ ONNX (48.4MB) and kmodel (49MB) generation working
   - **Size Problem**: 🚨 Model 3x too large for Flash (49MB vs 16MB limit)
   - **Memory Problem**: 🚨 Runtime 10x over RAM limit (59MB vs 6MB limit)
   - **Next Solution**: YOLOv5n with 6.7x fewer parameters (1.9M vs 12.66M)
   - **Class Strategy**: Consider reduction to 151 Pokemon or hierarchical approach
   - **Infrastructure**: ✅ Complete pipeline ready for smaller models

## YOLOv5n K210 Implementation Rules (IMPLEMENTED & WORKING)

### YOLOv5n Training Pipeline Implementation (✅ COMPLETED)
1. **Infrastructure Created (✅ FULLY IMPLEMENTED)**
   - **Training Script**: `train_yolov5n_k210.py` - ✅ Created with YOLOTrainer integration
   - **Model Config**: `yolov5n_k210_optimized.yaml` - ✅ YOLOv5n with aggressive parameters
   - **Data Config**: `yolov5n_k210_data.yaml` - ✅ All 1025 Pokemon classes maintained
   - **Checkpoint Isolation**: ✅ Fixed model-specific checkpoint detection logic
   - **W&B Integration**: ✅ Separate runs for model comparison

2. **Model Architecture Validation (✅ CONFIRMED)**
   - **Model**: YOLOv5n (modern 2020 architecture) - ✅ Successfully loaded
   - **Parameters**: 3.12M (vs YOLOv3-tiny's 12.66M = 4.1x reduction)
   - **Layers**: 153 modern layers (vs 53 old layers in YOLOv3-tiny)
   - **GFLOPs**: 10.3 (vs YOLOv3-tiny's 20.1 = 50% more efficient)
   - **Detection Head**: Modern 3-scale detection (vs 2-scale in YOLOv3-tiny)

3. **Parameter Transfer Strategy (✅ APPLIED)**
   - **Learning Rate**: 1e-3 (aggressive, proven successful with YOLOv3-tiny)
   - **Augmentation**: mosaic=0.5, mixup=0.3 (enabled for better generalization)
   - **Optimizer**: SGD forced to prevent auto-override of learning rate
   - **Early Stopping**: patience=10 (faster overfitting detection)
   - **Input Size**: 224x224 (K210 optimized)
   - **Batch Size**: 8 (memory efficient)

4. **Training Infrastructure Rules (✅ VALIDATED)**
   - **YOLOTrainer Integration**: ✅ Same infrastructure as YOLOv3-tiny
   - **Auto-backup**: ✅ Every 30 minutes to Google Drive via YOLOTrainer
   - **W&B Logging**: ✅ pokemon-classifier project for direct comparison
   - **Resume Logic**: ✅ Model-specific checkpoint detection (no cross-contamination)
   - **Class Maintenance**: ✅ All 1025 Pokemon classes preserved

5. **Model-Specific Checkpoint Detection (🔧 CRITICAL FIX)**
   - **Problem Fixed**: YOLOv5n was incorrectly resuming from YOLOv3-tiny checkpoints
   - **Solution Applied**: Model-aware checkpoint detection in YOLOTrainer
   - **Implementation**: Added `model_name` checking in `_prepare_training_args`
   - **Result**: YOLOv5n and YOLOv3-tiny now have isolated checkpoint spaces
   - **Validation**: Fresh YOLOv5n training confirmed (no incorrect resume)

6. **K210 Deployment Projections (📊 HIGHLY PROMISING)**
   - **Pre-quantization**: ~12-15MB expected (vs YOLOv3-tiny's 49MB)
   - **Post-INT8 quantization**: ~3-4MB projected (within K210 constraints)
   - **Runtime Memory**: ~6MB or less expected (within K210 RAM limit)
   - **Quantization Support**: Superior to YOLOv3-tiny (modern architecture)
   - **K210 Compatibility**: High confidence in successful deployment

