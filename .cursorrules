# Pokemon Classifier Project Rules

## Google Colab Training Environment

### Environment Setup
1. **Conda Installation (Ephemeral)**
   - Install to: `/content/miniconda3/` (reinstalled per session)
   - Environment: `pokemon-classifier` with Python 3.9
   - Activation: `source conda.sh && conda activate pokemon-classifier`
   - Shell script: `activate_env.sh` for easy activation
   - No Google Drive persistence due to file permission issues

2. **Dynamic Path Management**
   - Conda path detection: Check `/content/miniconda3/bin/conda` first
   - Function: `get_conda_path()` in multiple scripts
   - Primary path: `/content/miniconda3/bin/conda`
   - Fallback: PATH environment variable detection
   - PATH updates: Automatic conda binary detection

3. **Storage Strategy**
   - Repository: `/content/pokedex/pokedex/` for code and configs
   - Dataset: `/content/pokedex/pokedex/data/yolo_dataset/` from Hugging Face
   - Models: `/content/models/` directories (ephemeral)
   - Auto-creation: Required directories (`models/checkpoints`, etc.)
   - Error handling: I/O error retry logic for large file operations

## YOLO Training Rules

### Data Configuration
1. **YOLO Dataset Format Requirements**
   - Directory: `processed/yolo_dataset/`
   - Images: 416x416 JPEG in `images/{split}/`
   - Labels: YOLO format in `labels/{split}/`
   - Naming: `{pokemon_id}_{image_number:03d}.{ext}` (3-digit padding)
   - Class IDs: 0-based (0-1024)
   - Full-image boxes: "0.5 0.5 1.0 1.0"
   - Multiprocessing: 8 workers, 100 images per batch
   - Progress Tracking: Percentage complete per split
   - Lookup Tables: O(1) access for raw-to-processed mapping

2. **Dataset Verification**
   - Check Hugging Face access first
   - Verify YOLO config format
   - Validate class count (1025)
   - Check split names match
   - Verify per-Pokemon 70/15/15 splits
   - Validate label format and class IDs (0-based)
   - Check image sizes (416x416)
   - Verify image data types (bytes or PIL)
   - Skip already processed splits
   - Show progress with tqdm
   - Handle errors with descriptive messages
   - Update config paths dynamically

### Model Loading Strategy
1. **YOLOv3 Loading Priority**
   - **Primary**: Load official YOLOv3 weights (Ultralytics auto-download)
   - **Fallback**: Load from Ultralytics hub (`YOLO("yolov3")`)
   - YAML fallback removed (file `models/configs/yolov3.yaml` deleted)
   - **Cache Management**: Auto-cleanup corrupted weights
   - **Path Resolution**: Dynamic working directory detection

2. **Training Configuration**
   - Classes: 1025 (all Pokemon generations)
   - Input size: 416x416 pixels
   - Learning rate: 1e-4, cosine schedule with 5 warmup epochs

### Baseline Training Results & Learnings
1. **Performance Metrics (48 epochs completed)**
   - **Best mAP50**: 0.9746 (epoch 44)
   - **Best mAP50-95**: 0.803 (epoch 44)
   - **Training Loss**: Steady decrease until epoch 45
   - **Validation Loss**: Increasing trend after epoch 30 (overfitting)

2. **Critical Issues Identified**
   - **Training Instability**: Dramatic performance drop at epoch 45
     - mAP50 dropped from 0.9746 to 0.00041
     - mAP50-95 dropped from 0.803 to 0.00033
   - **Overfitting**: Validation loss increasing while training loss decreasing
   - **Learning Rate**: 1e-4 may be too high for 1025 classes
   - **Augmentation**: Minimal augmentation insufficient for generalization

3. **Improvement Opportunities**
   - **Early Stopping**: Implement to prevent overfitting (patience=10)
   - **Learning Rate**: Reduce to 5e-5 or implement adaptive scheduling
   - **Augmentation**: Add rotation, shear, mosaic, mixup
   - **Regularization**: Add dropout, weight decay, label smoothing
   - **Batch Size**: Consider increasing to 32 for better gradient estimates
   - **Monitoring**: Add validation metrics monitoring for early detection

4. **Baseline Configuration (Current)**
   - Learning rate: 1e-4 (cosine schedule)
   - Batch size: 16
   - Warmup epochs: 5
   - Augmentation: Horizontal flip only (0.5 probability)
   - No early stopping
   - No additional regularization

### Training Process
1. **Initialization Order**
   - Verify environment setup and conda activation
   - Check dataset access from Hugging Face
   - Initialize W&B tracking with run resumption
   - Set up model with fallback loading strategy
   - Create required directories automatically

2. **Checkpoint Management**
   - Save metadata with checkpoints (W&B run ID, epoch, metrics)
   - Track W&B run ID for resumption
   - Record actual progress vs saved epochs
   - Handle mid-epoch interruptions
    - Local storage (ephemeral per Colab session)
    - Auto-backup of training outputs to Google Drive every 30 minutes
    - Resume scans multiple default paths for latest checkpoint:
      - Custom checkpoints directory
      - `pokemon-classifier/<run-name>/weights`
      - `pokemon-yolo-training/<run-name>/weights`

3. **Error Handling**
   - File I/O errors: Retry with exponential backoff for large operations
   - Missing conda: Dynamic path detection and helpful messages
   - Model loading failures: Multiple fallback strategies
   - Dataset access: Hugging Face authentication and caching

### W&B Integration
1. **Configuration**
   - Use singleton pattern for initialization
   - Enable offline fallback with environment variables
   - Disable code/git tracking for Colab
   - Use personal account (liuhuanjim013-self)
   - Project: "pokemon-classifier"

2. **Metrics Logging**
   - **Built-in Ultralytics**: Automatic loss, mAP, precision, recall
   - **Real-time Monitoring**: Live training progress (callbacks on epoch/val end)
   - **System Metrics**: GPU usage, memory consumption
   - **Custom Logging**: Configuration parameters and experiment metadata
   - **Run Persistence**: Save run ID to disk for resumption

3. **Resume Strategy**
   - Try specified checkpoint with matching W&B run ID
   - Match W&B run ID from saved checkpoint metadata
   - Fall back to latest checkpoint if run ID missing
   - Support forcing new run with --force-new-run flag
   - Maintain metrics continuity during resume

4. **Error Handling**
   - Clean up W&B runs on training failure
   - Proper exception handling with informative messages
   - Resource cleanup and session management
   - Offline mode fallback for network issues

### Testing Requirements
1. **Training Tests**
   - Verify setup works
   - Check arguments
   - Test data loading
   - Validate progress

2. **W&B Tests**
   - Test initialization
   - Verify resumption
   - Check offline mode
   - Validate cleanup

3. **Checkpoint Tests**
   - Test metadata saving
   - Verify loading
   - Check progress tracking
   - Test interruption recovery

4. **Dataset Format Tests**
   - Verify image sizes (416x416)
   - Check label format
   - Validate class IDs (0-based)
   - Verify per-Pokemon splits
   - Test image-label pairs match

## K210 Export Pipeline Rules

### K210 Export & Deployment Rules

1. **Model Size & Memory Constraints**
   - **Hardware Limits**:
     - RAM: ~6MB maximum
     - Flash: ~16MB maximum
   - **Target Metrics**:
     - Model size: 1-2MB
     - Runtime memory: 2-3MB
     - Input size: 224x224 or smaller
     - Architecture: Tiny/efficient variants

2. **nncase Version & API Rules**
   - **Version**: Use nncase v1.6.0 (confirmed working)
   - **API Approach**: Use Python API directly, avoid CLI
   - **Compilation Flow**: 
     1. Import ONNX model
     2. Configure compiler options
     3. Compile model
     4. Generate kmodel using multiple approaches
   - **Error Handling**: Try multiple gencode approaches in order:
     1. BytesIO buffer with gencode_tobytes
     2. Direct bytes with gencode_tobytes
     3. String path with gencode (fallback)

3. **Environment Setup Requirements**
   - **Installation**: Use `setup_environment.py` with `--k210-only` flag
   - **Dependencies**: Install onnx, onnxruntime, onnxsim, nncase
   - **Version Management**: Handle nncase version conflicts
   - **Verification**: Test compilation with tiny model

4. **Export Pipeline Requirements**
   - **ONNX Export**: Use Ultralytics with fixed input shape
   - **Calibration**: Prepare 400 images at target resolution
   - **Memory Analysis**: Report detailed memory usage breakdown
   - **Artifacts**: Package kmodel with classes.txt and anchors.txt

5. **Model Optimization Rules**
   - **Architecture**: Use YOLOv3-tiny or smaller variant
   - **Resolution**: Target 224x224 input size
   - **Classes**: Consider grouping similar Pokemon
   - **Optimization**: Use channel pruning and aggressive quantization
   - **Memory**: Optimize intermediate feature map sizes
   - **Batch Size**: Keep at 1 for inference
   - **Layers**: Use memory-efficient layer configurations